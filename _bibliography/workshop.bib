

@article{rds2022,
      title={Enhancing Unit-tests for Invariance Discovery}, 
      author={Piersilvio De Bartolomeis and Antonio Orvieto and  Giambattista Parascandolo},
      year={2022},
      conference={Spurious Correlations, Invariance, and Stability Workshop (ICML),},
      pdf={https://openreview.net/pdf?id=-XVMGVmjNLs},
      abstract={Recently, Aubin et al. (2021) proposed a set of
linear low-dimensional problems to precisely evaluate different types of out-of-distribution generalization. In this paper, we show that one of these
problems can already be solved by established
algorithms, simply by better hyper-parameter tuning. We then propose an enhanced version of
the linear unit-tests. To the best of our hyperparameter search and within the set of algorithms
evaluated, AND-mask is the best performing algorithm on this new suite of tests. Our findings
on synthetic data are further reinforced by experiments on an image classification task where we
introduce spurious correlations.},
      archivePrefix={}     
}

@article{debartolomeis2023how,
      title={How robust accuracy suffers from certified training with convex relaxations}, 
      author={Piersilvio De Bartolomeis and Jacob Clarysse and Amartya Sanyal and Fanny Yang},
      year={2023},
      conference={I Can't Believe It's Not Better Workshop (NeurIPS),},
      abstract={Adversarial attacks pose significant threats to deploying state-of-the-art classifiers in safety-critical applications. Two classes of methods have emerged to address this issue: empirical defences and certified defences. Although certified defences come with robustness guarantees, empirical defences such as adversarial training enjoy much higher popularity among practitioners. In this paper, we systematically compare the standard and robust  error of these two robust training paradigms across multiple computer vision tasks. We show that in most tasks and for both \(\ell_\infty\)-ball and \(\ell_2\)-ball threat models, certified training with convex relaxations suffers from worse standard and robust error than adversarial training. We further explore how the error gap between certified and adversarial training depends on the threat model and the data distribution. In particular, besides the perturbation budget, we identify as important factors the shape of the perturbation set and the implicit margin of the data distribution. We support our arguments with extensive ablations on both synthetic and image datasets.},
      arxiv={https://arxiv.org/abs/2306.06995}
}

