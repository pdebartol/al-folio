@article{pier2022certified,
      title={{Why certified defences hurt robust generalisation}}, 
      author={Piersilvio De Bartolomeis and Jacob Clarysse and Fanny Yang and Amartya Sanyal},
      year={2022},
      conference={Understanding Deep Learning Through Empirical Falsification Workshop (NeurIPS),},
      award={Oral},
     pdf={certified_defences.pdf},
     poster={poster_ICBINB.pdf},
     slides={talk_neurips_ICBINB.pdf},
     abstract={Several works have shown that state-of-the-art classifiers are vulnerable to adversarial examples, raising serious concerns for their deployment in safety-critical applications. To address this issue two classes of methods have emerged: empirical defences and certified defences. While the former has good robustness, but no guarantees, the latter sacrifices some robustness in exchange for guarantees. Until now, these two approaches have not been systematically compared in the literature, and a clear understanding of the robustness gap remains elusive. In this paper, we show through extensive empirical evidence that models trained with certified defences suffer from worse accuracy, robustness and fairness than empirical defences. Further, we identify three key factors contributing to the robustness gap between the two approaches and support our arguments with both theoretical and experimental evidence. We hope this serves as a guide to practitioners regarding the use of various defences as well as a motivation for designing future methods.}
}
}


@article{mutti2022challenging,
      title={Challenging Common Assumptions in Convex Reinforcement Learning}, 
      author={Mirco Mutti* and Riccardo De Santi* and Piersilvio De Bartolomeis and Marcello Restelli},
      year={2022},
     conference = {Advances in {Neural} {Information} {Processing} {Systems} (NeurIPS),},
      abstract={The classic Reinforcement Learning (RL) formulation concerns the maximization of a scalar reward function. More recently, convex RL has been introduced to extend the RL formulation to all the objectives that are convex functions of the state distribution induced by a policy. Notably, convex RL covers several relevant applications that do not fall into the scalar formulation, including imitation learning, risk-averse RL, and pure exploration. In classic RL, it is common to optimize an infinite trials objective, which accounts for the state distribution instead of the empirical state visitation frequencies, even though the actual number of trajectories is always finite in practice. This is theoretically sound since the infinite trials and finite trials objectives are equivalent and thus lead to the same optimal policy. In this paper, we show that this hidden assumption does not hold in convex RL. In particular, we prove that erroneously optimizing the infinite trials objective in place of the actual finite trials one, as it is usually done, can lead to a significant approximation error. Since the finite trials setting is the default in both simulated and real-world RL, we believe shedding light on this issue will lead to better approaches and methodologies for convex RL, impacting relevant research areas such as imitation learning, risk-averse RL, and pure exploration among others.},
      arxiv={https://arxiv.org/abs/2202.01511},
      poster={neurips_convex_poster.pdf}
}


@article{rds2022,
      title={Enhancing Unit-tests for Invariance Discovery}, 
      author={Piersilvio De Bartolomeis and Antonio Orvieto and  Giambattista Parascandolo},
      year={2022},
      conference={Spurious Correlations, Invariance, and Stability Workshop (ICML),},
      pdf={https://openreview.net/pdf?id=-XVMGVmjNLs},
      abstract={Recently, Aubin et al. (2021) proposed a set of
linear low-dimensional problems to precisely evaluate different types of out-of-distribution generalization. In this paper, we show that one of these
problems can already be solved by established
algorithms, simply by better hyper-parameter tuning. We then propose an enhanced version of
the linear unit-tests. To the best of our hyperparameter search and within the set of algorithms
evaluated, AND-mask is the best performing algorithm on this new suite of tests. Our findings
on synthetic data are further reinforced by experiments on an image classification task where we
introduce spurious correlations.},
      archivePrefix={}     
}

